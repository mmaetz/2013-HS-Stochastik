\chapter{Gemeinsame und bedingte stetige Verteilungen}
\label{kap6}
Bei zwei oder mehreren stetigen Zufallsvariablen kann die gemeinsame und bedingte Verteilung nicht mehr mit Bäumen oder Matrizen dargestellt werde wie in Kapitel 5.
Bei zwei oder mehreren stetigen Zufallsvariablen kann die gemeinsame und bedingte Verteilung nicht mehr mit Bäumen oder Matrizen dargestellt werden wie in Kapitel \ref{kap5}.
\section{Gemeinsame Dichte}
Die gemeinsame Dichte $f_{x,Y}(\cdot,\cdot)$ von zwei stetigen Zufallsvariablen $X$ und $Y$ ist gegeben, in \glqq Ingenieurnonation\grqq, durch
\begin{multline*}
	\p(x\leq X\leq x+\dd{x}, y\leq Y\leq y+\dd{y})\\=f_{X,Y}(x,y)\rd x \rd y.
\end{multline*}
(Die Darstellugn als Ableitung einer geeigneten kumulativen Verteilungsfunktion ist nicht sehr instruktiv.) daraus kann man allgemein Wahrscheinlichkeiten durch Integration berechnen:
\begin{gather*}
\p\!\left( (X,Y)\in A \right)=\iint_{A}\!\dd{x}\dd{y}\, f_{X,Y}(x,y)
\end{gather*}
\section{Randdichte und bedingte Dichte}
Aus der gemeinsamen Dichte erhält man insbesondere die Randdichte von $X$ bzw. $Y$
\begin{align*}
	f_{X}(x)&=\int_{-\infty}^{\infty}\!\dd{y}\, f_{X,Y}(x,y),\\
	f_{Y}(y)&=\int_{-\infty}^{\infty}\! \dd{x}\, f_{X,Y}(x,y).
	\intertext{Für die bedingte Verteilung von $Y$ gegeben $X=x$ wird die bedingte Dichte benützt, definiert durch}
	f_{Y\mid X=x}(y)&\coloneqq f_{Y}(y\mid X=x)\\
	&\coloneqq \frac{f_{X,Y}(x,y)}{f_{X}(x)}
\end{align*}
Aus den obigen Definitionen ist klar, dass all wahrscheinlichkeitstheoretischen Aspekte von 2 Zufallsvariablen $X$ und $Y$ durch deren gemeinsame Dichte $f_{X,Y}$ vollständig bestimmt sind. $X$ und $Y$ sind unabhängig genau dann wenn
\begin{gather}
	f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y), \qquad x,y \in \mathbb{R}^2.
	\label{eq:unabh}
\end{gather}
In diesem Fall genügt das Konzept von 1-dimensionalen Dichten: die gemeinsame Dichte kann dann sehr einfach mittels Multiplikation berechnet werden.
\section{Erwartungswert bei mehreren Zufallsvariablen}
Der Erwartungswert macht nur Sinn für eine $\mathbb{R}$-wertige Gröss (oder Teilmenge von $\mathbb{R}$). Den Erwartungswert einer transformierten Zufallsvariable $Z=g(X,Y)$ mit $g:\mathbb{R}^2\to \mathbb{R}$ können wir berechnen als
\begin{align*}
	\E(g(X,Y))&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\!\rd x\rd y \, g(x,y)f_{X,Y}(x,y)
	\intertext{Im diskreten Fall lautet die entrpechende Formel:}
	\E(g(X,Y))&=\sum_{i,j}^{}g(x_i,y_j)\p(X=x_i,Y=y_j).
	\intertext{Der Erwartungswert von der einen Zufallsvariablen $Y$ gegen $X=x$ ist gegeben durch}
	\E(Y\mid X=x)&=\int_{-\infty}^{\infty}\rd y \, y f_{Y\mid X=x}(y)
\end{align*}
\section{Kovarianz und Korrelation}
Da die gemeinsame Verteilung von abhängigen Zufallsvariablen i.A. kompliziert ist, begnügt man sich oft mit einer \emph{vereinfachenden} Kennzahl zur Beschreibung der Abhängigkeit. Die \emph{Kovarianz von $X$ und $Y$} sowie \emph{Korrelation von $X$ und $Y$} sind wie folgt definiert:
\begin{align*}
	V(x)=\cov(X,X),
	\corr(X,Y)&\coloneqq \rho_{XY},\\
	&\coloneqq \frac{\cov(X,Y)}{\sigma_{X}\sigma_{Y}}
\end{align*}
Unmittelbar aus der Definition folgt sofort
\begin{align*}
	\cov(X,Y)=\E(XY)-\E(X)\E(Y),
	\intertext{sowie di wichtige Formel}
	\cov(X,Y)=\E(XY)-\E(X)\E(Y),
\end{align*}
zur praktischen Berechnung der Kovarianz. Weiter ist die Kovarianz \emph{bilinear}, d.h. es gilt
\begin{multline*}
	\cov\!\left( \sum_{i=1}^{m}a_iX_i,\sum_{j=1}^{n}b_jY_j \right)\\
	=\sum_{i=1}^{m}\sum_{j=1}^{n}a_ib_j\cov(X_i,Y_j)
\end{multline*}
und symmetrisch, d.h.
\begin{gather*}
	\cov(X,Y)=\cov(Y,X)
\end{gather*}
womit wir nun auch in der Lage sind die Varianz von Linearkombinationen von Zufallsvariablen elegant auszudrücken, es gilt nämlich
\begin{align*}
	\V\!\left( \sum_{i=1}^{m}X_i \right)
	&=\sum_{i=1}^{m}\V(X_i)\\&\quad+2\sum_{\substack{i,j=1\\i<j}}^{m}\cov(X_i,X_j).
\end{align*}
falls die $X_i$ unabhängig (oder noch allgemeiner unkorreliert) sind vereinfacht sich diese Formel zu
\begin{align*}
	\V(X_1+\cdots+X_m)&=\V(X_1)+\cdots +\V(X_m)
	\intertext{$X_1,\ldots,X_n$ \emph{unabhängig}.}
	\cov(a+bX,c+dY)&=bd\cov(X,Y),\\
	\corr(a+bX,c+dY)&=\sgn(b)\sgn(d)\\
	&\quad\cdot\corr(X,Y);\\
	V(X+Y)&=\V(X)+\V(Y)\\
	&\quad+2\cov(X,Y)
\end{align*}
Die Korrelation misst Stärke und Richtung der \emph{linearen Abhängigkeit zwischen $X$ und $Y$}. Es gilt
\begin{align*}
	\corr(X,Y)&=+1 &\Leftrightarrow \quad Y&=a+bX
	\shortintertext{$\forall a\in \mathbb{R}, b>0$}
	\corr(X,Y)&=-1 &\Leftrightarrow \quad Y&=a+bX
\end{align*}
	$\forall a\in \mathbb{R}, b<0$.\\
Überdies gilt
\begin{gather}
	X\text{ und }Y\text{ unabhängig} \quad\Rightarrow\quad \corr(X,Y)=0.
	\label{eq:unabhtocorr}
\end{gather}
Die Umkehrung gilt i.A. nicht. Ein Spezialfall, wo auch die Umkehrung gilt, wird in Kapitel \ref{sec6.6} diskutiert.
\section{Linear Prognose}
Bei der linearen Prognose von $Y$ gestützt auf $X$ macht man den Ansatz
\begin{gather*}
	\hat{Y}=a+bX
\end{gather*}
und bestimmt die Koeffizienten so, dass der mittlere quadratische Prognosefehler $\E\!\left(\left( Y-\hat{Y} \right)^2  \right)$ minimal wird. Man kann zeigen dass die Lösung dieses Optimierungsproblems gegeben ist durch
\begin{align*}
	\hat{Y}&=\mu_{Y}+\frac{\cov(X,Y)}{\V(X)}(X-\mu_X)\\
	\E\!\left(\left( Y-\hat{Y} \right)^2  \right)
	&=\left( 1-\rho_{XY}^{2} \right)\V(Y).
\end{align*}
\section{Zwei-dimensionale Normalverteilung}
\label{sec6.6}
Die wichtigste zweidimensionale Verteilung ist die Normalverteilung mit Erwartungswerten $(\mu_{X},\mu_Y)$ und Kovarianzmatrix $\Sigma$, wobei
\begin{gather*}
	\Sigma=
	\begin{pmatrix}
		\V(X)&\cov(X,Y)\\
		\cov(X,Y)&\V(Y)
	\end{pmatrix}.
	\end{gather*}
Sie hat die Dichte
\begin{multline*}
	f_{X,Y}(x,y)=\frac{1}{2\pi\sqrt{\det(\Sigma)}}\\
	 \cdot \exp\!\left( -\frac{1}{2}\left( x-\mu_X,y-\mu_Y \right)\Sigma^{-1}
	\begin{pmatrix}
		x-\mu_X\\
		y-\mu_Y
	\end{pmatrix}
	\right)
\end{multline*}
Wir sehen von dieser Formel: Wenn 
\begin{gather*}
	\cov(X,Y)=0
\end{gather*}
wird $\Sigma$ eine Diagonalmatrix und man kann nachrechnen dass dann die Bedingung Gl. \ref{eq:unabh} gilt. Das heisst: Im Falle der zwei-dimensionalen Normalverteilung gilt auch die Umkehrung von Gl. \ref{eq:unabhtocorr}. Zudem: Die Rand- und bedingten Verteilungen sind wieder (1-dimensional) normal.
\section{Mehr als 2 Zufallsvariablen}
Alle diese Begriffe und Definitionen lassen sich natürlich auf mehr als zwei Zufallsvariablen verallgemeinern. Die Formeln sehen im wesentlichen gleich aus, vor allem wenn man die Sprache der Linearen Algebra verwendet.

\emph{Ausblick:} Wenn man eine dynamische Grösse während eines Zeitintervalls misst, erhält man einen stochastischen Prozess $\left\{ X(t); t\in \left[ a,b \right] \right\}$. Die linearen Abhängigkeiten zwischen den Werten zu verschiedenen Zeitpunkten werden dann durch die sogenannte \emph{Autokovarianzfunktion} beschrieben.
