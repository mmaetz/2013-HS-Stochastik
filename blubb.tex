\chapter{Punktschätzungen}
\label{kap10}
Wir betrachten folgendes Problem: Gegeben sei wieder eine Zufallsvariable $X$ von der wir zwar die Art der Verteilung kennen, aber nicht den dazugehörigen Parameter (Vektor) $\theta$. basierend auf $n$ unabhängigen Beobachtungen $x_1,\ldots,x_n$ von $X$, die wir wie immer als eine Realisierung der i.i.d. Zufallsvariablen $X_1,\ldots,X_n$ (mit derselben Verteilung wie $X$) interpretieren soll nun der unbekannte Parameter $\theta$ geschätzt werden. Sei $\theta$ ein reeller Parameter einer Wahrscheinlichkeitsverteilung auf $\mathbb{R}$, ein \emph{Schätzer für} $\theta$ (zur Stichprobengrösse $n$) ist eine Funktion $\hat{\theta}: \mathbb{R}^n\to \mathbb{R}$ von den $\mathbb{R}^n$ 
\begin{align*}
	\hat{\theta}&=\hat{\theta}(x_1,\ldots,x_n)
	\intertext{oder als Zufallsvariable interpretiert}
	\hat{\theta}&=\hat{\theta}(X_1,\ldots,X_n).
\end{align*}
Man beachte dass bei der Verwendung von griechischen buchstaben keine Unterscheidung mehr zwischen Realisierung (Kleinbuchstaben) und Zufallsvariable (Grossbuchstaben) gemacht wird.
\section{Momentenmethode}
Das $k$\emph{-te Moment von} $X$ ist definiert durch 
\begin{align}
	\mu_k&\coloneqq \E(X^k),\nonumber
	\intertext{das erste Moment entspricht also dem Erwartungswert. Die Momentenmethode nimmt an, dass wir den unbekannten Parametervektor $(\theta_1,\ldots,\theta_r)$ durch die ersten $p$ Momente von $X$ ausdrücken können, d.h. allgemein}
	\theta_j&=g_j(\mu_1,\ldots,\mu_p), \qquad j=1,\ldots,r,
	\label{eq:above}
\end{align}
die $j$-te Komponente von $\theta$ ist also gegeben durch die Funktion $g_j:\mathbb{R}^p\to\mathbb{R}$.

Ausgehend von Gleichung \ref{eq:above} werden nun die wahren $\mu_k$ ersetzt durch deren Schätzungen 
\begin{align*}
	\hat{\mu}_k&\coloneqq\frac{1}{n}\sum_{i=1}^{n}x_{i}^{k},
	\intertext{wenn man diese noch als Zufallsvariable schreibt, dann erhält man einen Momentenschätzer für die einzelnen Komponenten von $\theta$ durch}
	\hat{\theta}_j&=g_j(\hat{\mu}_(,\ldots,\hat{\mu}_p),& j&=1,\ldots,r;\\
	\hat{\mu}_k&=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k},& k&=1,\ldots,p.
\end{align*}
\emph{Rezeptatrige Vorgangsweise:} 
\begin{compactenum}[1.]
	\item Gegeben ist eine Zufallsvariable $X$ deren Verteilung von einem unbekannten (zu schätzenden) Parameter $\theta$ abhängt. 
	\item Man berechnet zunächst das erste Moment von $X$, wenn dieses von $\theta$ abhängt, dann löst man die Gleichung nach $\theta$ auf, d.h. man schreibt $\theta$ als Funktion des ersten Momentes.
	\item Dann wird das erste Moment durch dessen Schätzer ersetzt und man erhält einen Momentenschätzer für $\theta$. 
	\item Falls das erste Moment nicht von $\theta$ abhängt berechnet man das zweite Moment usw. Es ist aber auch möglich, dass sämtliche Momente von $\theta$ abhängen, man erhält dann mehrere (unterschiedliche) Momentenschätzer für $\theta$.
\end{compactenum}
\begin{bspl}
	Sei $X\sim\poi(\lambda)$ mit unbekanntem Parameter $\lambda$. Das erste Moment von $X$ ist
	\begin{align*}
		\E(X)&=\lambda,
		\intertext{oder anders geschrieben,}
		\lambda&=\mu_1,
		\intertext{nun wird $\mu_1$ durch dessen Schätzung}
		\hat{\mu}_1&=\frac{1}{n}\sum_{i=1}^{n}x_i
		\intertext{ersetzt, als Zufallsvariable geschrieben erhält man einen Momentenschätzer für $\lambda$ durch}
		\hat{\lambda}&=\hat{\mu}_1=\overline{X}_n=\frac{1}{n}\sum_{i=1}^{n}X_i.
	\end{align*}
\end{bspl}
Einen weiteren Momentenschätzer fur $\lambda$ erhalten wir durch Berechnung des zweiten Momentes von 
\begin{align*}
	X:\mu_2&=\E(X^2)=\V(X)+\E(X)^2=\lambda+\mu_{1}^{2}
	\intertext{woraus nun}
	\lambda&=\mu_2-\mu_{1}^{2}
	\intertext{folgt und wir erhalten den Momentenschätzer}
	\hat{\lambda}&=\hat{\mu}_2-\hat{\mu}_{1}^{2}\\
	&=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}\\
	&=\frac{n-1}{n}S_{n}^{2}.
\end{align*}
???LINES MISSING
Wir haben gesehen, dass ein Schätzer für den Parameter $\theta$ als eine aus den Zufallsvariablen $X_1,\ldots,X_n$ zusammengesetzte Zufallsvariable interpretiert werden kann, also $\hat{\theta}=\hat{\theta}(X_1,\ldots,X_n)$ wobei der zu schätzende Parameter $\theta$ meist eine Kenngrösse der Verteilung der $X_i$ ist. Man kann sich nun fragen wie eigentlich der Erwartungsswert dieser Zufallsvariable (also des Schätzers) aussieht, intuitiv wünschenswert wäre es wann dieser am besten gleich dem zu schätzenden Parameter entspricht, woraus sich folgende Eigenschaft ergibt: Ein Schätzer $\hat{\theta}$ für den Parameter $\theta$ heisst \emph{erwartungstreu} wenn
\begin{gather*}
	\E(\hat{\theta})=\E\left( \hat{\theta}(X_1,\ldots,X_n) \right)=\theta.
\end{gather*}
Im Allgemeinen kann der Erwartungswert aber auch von der Stichprobengrösse $n$ abhängen, woraus sich noch weiter Eigenschaft ergeben: z.B. heisst ein Schätzer \emph{asymptotisch erwartungstreu}, wenn sein Erwartungswert für wachsende Stichprobengrösse $n$ gegen den zu schätzenden Paremeter konvergiert.
